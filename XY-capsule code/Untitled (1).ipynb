{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e48754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610ebdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8508b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.301023</td>\n",
       "      <td>-1.651728</td>\n",
       "      <td>-1.538896</td>\n",
       "      <td>-1.562533</td>\n",
       "      <td>-0.329070</td>\n",
       "      <td>0.350222</td>\n",
       "      <td>-0.391974</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.301023</td>\n",
       "      <td>-1.651728</td>\n",
       "      <td>-1.538896</td>\n",
       "      <td>-1.562533</td>\n",
       "      <td>-0.329070</td>\n",
       "      <td>0.350222</td>\n",
       "      <td>-0.391974</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.301023</td>\n",
       "      <td>-1.651728</td>\n",
       "      <td>-1.538896</td>\n",
       "      <td>-1.562533</td>\n",
       "      <td>-0.329070</td>\n",
       "      <td>0.350222</td>\n",
       "      <td>-0.391974</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.301023</td>\n",
       "      <td>-1.651728</td>\n",
       "      <td>-1.538896</td>\n",
       "      <td>-1.562533</td>\n",
       "      <td>-0.329070</td>\n",
       "      <td>0.350222</td>\n",
       "      <td>-0.391974</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.301023</td>\n",
       "      <td>-1.651728</td>\n",
       "      <td>-1.538896</td>\n",
       "      <td>-1.275513</td>\n",
       "      <td>-0.329070</td>\n",
       "      <td>0.348971</td>\n",
       "      <td>-0.428100</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>11995</td>\n",
       "      <td>1.181845</td>\n",
       "      <td>1.620713</td>\n",
       "      <td>1.450329</td>\n",
       "      <td>1.594679</td>\n",
       "      <td>-0.368392</td>\n",
       "      <td>-0.005276</td>\n",
       "      <td>-0.428100</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>1.181845</td>\n",
       "      <td>1.620713</td>\n",
       "      <td>1.586202</td>\n",
       "      <td>-1.562533</td>\n",
       "      <td>-0.368392</td>\n",
       "      <td>-0.004337</td>\n",
       "      <td>-0.428100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>1.181845</td>\n",
       "      <td>1.620713</td>\n",
       "      <td>1.586202</td>\n",
       "      <td>-1.275513</td>\n",
       "      <td>-0.368392</td>\n",
       "      <td>-0.004963</td>\n",
       "      <td>-0.428100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>1.181845</td>\n",
       "      <td>1.620713</td>\n",
       "      <td>1.586202</td>\n",
       "      <td>0.159583</td>\n",
       "      <td>-0.368392</td>\n",
       "      <td>-0.005901</td>\n",
       "      <td>-0.428100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>11999</td>\n",
       "      <td>1.181845</td>\n",
       "      <td>1.620713</td>\n",
       "      <td>1.586202</td>\n",
       "      <td>1.307660</td>\n",
       "      <td>-0.368392</td>\n",
       "      <td>-0.004650</td>\n",
       "      <td>-0.428100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0               0 -1.301023 -1.651728 -1.538896 -1.562533 -0.329070  0.350222   \n",
       "1               1 -1.301023 -1.651728 -1.538896 -1.562533 -0.329070  0.350222   \n",
       "2               2 -1.301023 -1.651728 -1.538896 -1.562533 -0.329070  0.350222   \n",
       "3               3 -1.301023 -1.651728 -1.538896 -1.562533 -0.329070  0.350222   \n",
       "4               4 -1.301023 -1.651728 -1.538896 -1.275513 -0.329070  0.348971   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "11995       11995  1.181845  1.620713  1.450329  1.594679 -0.368392 -0.005276   \n",
       "11996       11996  1.181845  1.620713  1.586202 -1.562533 -0.368392 -0.004337   \n",
       "11997       11997  1.181845  1.620713  1.586202 -1.275513 -0.368392 -0.004963   \n",
       "11998       11998  1.181845  1.620713  1.586202  0.159583 -0.368392 -0.005901   \n",
       "11999       11999  1.181845  1.620713  1.586202  1.307660 -0.368392 -0.004650   \n",
       "\n",
       "              6  label  \n",
       "0     -0.391974     11  \n",
       "1     -0.391974     11  \n",
       "2     -0.391974     11  \n",
       "3     -0.391974     11  \n",
       "4     -0.428100     11  \n",
       "...         ...    ...  \n",
       "11995 -0.428100     14  \n",
       "11996 -0.428100      0  \n",
       "11997 -0.428100      0  \n",
       "11998 -0.428100      0  \n",
       "11999 -0.428100      0  \n",
       "\n",
       "[12000 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../capsule_code/data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1304f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294fb554",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data[['0','1','2','3','4','5','6']]\n",
    "label = all_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24c2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size = 0.2, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "X_train, X_test, X_valid = np.array(X_train).reshape(-1, 7), np.array(X_test).reshape(-1, 7), np.array(X_valid).reshape(-1, 7)\n",
    "y_train, y_test, y_valid = np.array(y_train).reshape(-1, 1), np.array(y_test).reshape(-1, 1), np.array(y_valid).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f70ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train, X_valid, X_test = torch.FloatTensor(X_train), torch.FloatTensor(X_valid), torch.FloatTensor(X_test)\n",
    "y_train, y_valid, y_test = torch.FloatTensor(y_train), torch.FloatTensor(y_valid), torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beccea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 50,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 6,\n",
    "    'drop_last': False,\n",
    "    'num_epochs': 50,\n",
    "#     'encode_dim': 1,\n",
    "#     'hidden_dim': 128,\n",
    "#     'output_dim': 1,\n",
    "#     'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cab6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Dataset(Dataset):\n",
    "  # 'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data, labels):\n",
    "        # 'Initialization'\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a046f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(X_train, y_train)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=config['shuffle'],\n",
    "#     num_workers=config['num_workers'],\n",
    "    drop_last=config['drop_last']\n",
    ")\n",
    "\n",
    "validation_set = Dataset(X_valid, y_valid)\n",
    "validation_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=config['shuffle'],\n",
    "#     num_workers=config['num_workers'],\n",
    "    drop_last=config['drop_last']\n",
    ")\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=config['shuffle'],\n",
    "#     num_workers=config['num_workers'],\n",
    "    drop_last=config['drop_last']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffded06b",
   "metadata": {},
   "source": [
    "# 1.使用CNN训练异常分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2621ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # x: torch.Size([32, 7, 1])\n",
    "        # torch.Size([32, 16, 3])\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 7, out_channels = 64, kernel_size = 3, stride = 1, padding = 2), \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        \n",
    "        # torch.Size([32, 32, 5])\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 32, kernel_size = 2, stride = 1, padding = 1), \n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "#         # torch.Size([32, 64, 7])\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels = 32, out_channels = 64, kernel_size = 2, stride = 1, padding = 1), \n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.LeakyReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5)\n",
    "#         )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(64,16)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         print(\"x3:\",x.shape)\n",
    "#         x = self.layer4(x)\n",
    "#         print(\"x4:\",x.shape)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "#         return F.log_softmax(self.l5(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273835ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv1d(7, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1d(64, 32, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=True)\n",
       "    (3): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (4): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df3ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    batch_data = batch_data.unsqueeze(0)\n",
    "    batch_data = batch_data.permute(1,2,0)\n",
    "    output = cnn(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1364bc55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 16]),\n",
       " tensor([[0.0643, 0.0727, 0.0459, 0.0377, 0.0955, 0.0831, 0.0940, 0.0614, 0.0429,\n",
       "          0.0572, 0.0463, 0.0676, 0.0729, 0.0505, 0.0436, 0.0644],\n",
       "         [0.0674, 0.0383, 0.0720, 0.0496, 0.0685, 0.1158, 0.0495, 0.0692, 0.0518,\n",
       "          0.0894, 0.0573, 0.0678, 0.0489, 0.0319, 0.0661, 0.0565],\n",
       "         [0.0920, 0.0551, 0.0512, 0.0433, 0.0701, 0.1097, 0.0405, 0.0553, 0.0382,\n",
       "          0.0981, 0.0687, 0.0870, 0.0400, 0.0342, 0.0715, 0.0450],\n",
       "         [0.0751, 0.0542, 0.0776, 0.0733, 0.0411, 0.0401, 0.0657, 0.1110, 0.0792,\n",
       "          0.0518, 0.0217, 0.0443, 0.0296, 0.1466, 0.0328, 0.0560],\n",
       "         [0.0277, 0.0595, 0.0332, 0.0927, 0.0670, 0.0415, 0.0374, 0.0805, 0.1207,\n",
       "          0.0370, 0.0953, 0.0746, 0.1100, 0.0181, 0.0704, 0.0344],\n",
       "         [0.0292, 0.0140, 0.0539, 0.0244, 0.0154, 0.0260, 0.0396, 0.0160, 0.0282,\n",
       "          0.0315, 0.0529, 0.0173, 0.0099, 0.0074, 0.0917, 0.5424],\n",
       "         [0.0594, 0.0467, 0.0690, 0.0756, 0.0600, 0.0614, 0.0768, 0.0535, 0.1007,\n",
       "          0.0459, 0.0599, 0.0704, 0.0818, 0.0497, 0.0414, 0.0478],\n",
       "         [0.0824, 0.0624, 0.0721, 0.0542, 0.0580, 0.0316, 0.0774, 0.0281, 0.0632,\n",
       "          0.0663, 0.0241, 0.0729, 0.0909, 0.0952, 0.0396, 0.0817],\n",
       "         [0.0760, 0.0792, 0.0316, 0.0415, 0.0544, 0.0708, 0.0925, 0.0679, 0.0364,\n",
       "          0.0367, 0.0623, 0.0740, 0.0920, 0.0701, 0.0636, 0.0510],\n",
       "         [0.0384, 0.1390, 0.0734, 0.0757, 0.0168, 0.0381, 0.0192, 0.0553, 0.1502,\n",
       "          0.0472, 0.0327, 0.0137, 0.1503, 0.0564, 0.0401, 0.0536],\n",
       "         [0.0388, 0.0523, 0.0381, 0.0569, 0.0358, 0.0831, 0.0424, 0.0638, 0.0295,\n",
       "          0.2024, 0.0471, 0.0544, 0.0475, 0.0519, 0.0878, 0.0680],\n",
       "         [0.0548, 0.0639, 0.0537, 0.0688, 0.0757, 0.0593, 0.1295, 0.0486, 0.0499,\n",
       "          0.0396, 0.0331, 0.0541, 0.0601, 0.1032, 0.0452, 0.0605],\n",
       "         [0.0436, 0.0485, 0.0588, 0.0772, 0.0689, 0.0475, 0.0829, 0.0838, 0.0637,\n",
       "          0.0486, 0.0635, 0.0672, 0.0605, 0.0647, 0.0508, 0.0699],\n",
       "         [0.0758, 0.0525, 0.0469, 0.1037, 0.0438, 0.0789, 0.0657, 0.0732, 0.0669,\n",
       "          0.0461, 0.0795, 0.0715, 0.0547, 0.0580, 0.0334, 0.0493],\n",
       "         [0.1560, 0.0417, 0.0690, 0.0445, 0.0656, 0.0830, 0.0697, 0.0516, 0.0580,\n",
       "          0.0564, 0.0791, 0.0504, 0.0570, 0.0301, 0.0468, 0.0409],\n",
       "         [0.0453, 0.0433, 0.0320, 0.1108, 0.0284, 0.1041, 0.1419, 0.0539, 0.0372,\n",
       "          0.0410, 0.0184, 0.0705, 0.0504, 0.0400, 0.0759, 0.1068],\n",
       "         [0.0287, 0.0346, 0.0351, 0.0400, 0.1507, 0.0355, 0.0905, 0.0258, 0.0720,\n",
       "          0.0740, 0.0491, 0.0884, 0.1267, 0.0431, 0.0425, 0.0631],\n",
       "         [0.1360, 0.0389, 0.0526, 0.0074, 0.0514, 0.0292, 0.0589, 0.0316, 0.0876,\n",
       "          0.2217, 0.0328, 0.0336, 0.0367, 0.0184, 0.0287, 0.1347],\n",
       "         [0.0336, 0.0319, 0.0841, 0.0455, 0.0468, 0.0766, 0.0974, 0.1054, 0.0582,\n",
       "          0.0788, 0.0676, 0.0709, 0.0514, 0.0360, 0.0613, 0.0543],\n",
       "         [0.0512, 0.0966, 0.0335, 0.0379, 0.0461, 0.0880, 0.1748, 0.0609, 0.0343,\n",
       "          0.0485, 0.0284, 0.0312, 0.0505, 0.0478, 0.0919, 0.0785],\n",
       "         [0.0130, 0.0170, 0.0814, 0.0898, 0.0130, 0.0431, 0.0141, 0.0846, 0.0304,\n",
       "          0.1156, 0.0245, 0.0606, 0.0466, 0.0358, 0.2610, 0.0696],\n",
       "         [0.1328, 0.0705, 0.0610, 0.0233, 0.0388, 0.0282, 0.1119, 0.0430, 0.0977,\n",
       "          0.0632, 0.0709, 0.0659, 0.0223, 0.0255, 0.0176, 0.1272],\n",
       "         [0.0848, 0.0500, 0.0423, 0.0582, 0.0612, 0.0845, 0.0815, 0.0388, 0.0352,\n",
       "          0.0560, 0.0884, 0.1117, 0.0576, 0.0349, 0.0604, 0.0547],\n",
       "         [0.1102, 0.0427, 0.0522, 0.0897, 0.0420, 0.0602, 0.0809, 0.0858, 0.0577,\n",
       "          0.0524, 0.0422, 0.0542, 0.0670, 0.0415, 0.0798, 0.0415],\n",
       "         [0.2126, 0.0780, 0.1134, 0.0239, 0.0188, 0.0315, 0.0035, 0.0407, 0.2194,\n",
       "          0.0256, 0.0296, 0.0497, 0.0658, 0.0467, 0.0291, 0.0117],\n",
       "         [0.0817, 0.0577, 0.0627, 0.0441, 0.0969, 0.0849, 0.0557, 0.0567, 0.0519,\n",
       "          0.0500, 0.0945, 0.0674, 0.0631, 0.0408, 0.0479, 0.0440],\n",
       "         [0.0164, 0.0834, 0.0416, 0.1555, 0.0244, 0.0344, 0.0330, 0.0877, 0.1603,\n",
       "          0.0763, 0.0434, 0.0294, 0.0414, 0.0724, 0.0566, 0.0437],\n",
       "         [0.0731, 0.0483, 0.0289, 0.0850, 0.0584, 0.0415, 0.0226, 0.0644, 0.0570,\n",
       "          0.0428, 0.2087, 0.0856, 0.0735, 0.0497, 0.0341, 0.0263],\n",
       "         [0.0477, 0.0599, 0.0399, 0.0571, 0.0498, 0.0763, 0.0193, 0.0188, 0.0145,\n",
       "          0.1162, 0.0512, 0.1554, 0.1481, 0.0652, 0.0400, 0.0406],\n",
       "         [0.0538, 0.0630, 0.0159, 0.1866, 0.0411, 0.0960, 0.0741, 0.0435, 0.0445,\n",
       "          0.0571, 0.0734, 0.0224, 0.0385, 0.0168, 0.0995, 0.0739],\n",
       "         [0.0548, 0.0397, 0.0353, 0.1347, 0.0613, 0.0469, 0.0608, 0.0796, 0.0510,\n",
       "          0.0490, 0.0747, 0.0716, 0.0471, 0.0630, 0.0803, 0.0502],\n",
       "         [0.1054, 0.0429, 0.0532, 0.0615, 0.0962, 0.0585, 0.0655, 0.0530, 0.0695,\n",
       "          0.0465, 0.0602, 0.0544, 0.0774, 0.0641, 0.0550, 0.0367],\n",
       "         [0.0652, 0.0387, 0.0803, 0.0429, 0.0758, 0.0810, 0.1031, 0.0493, 0.0445,\n",
       "          0.0665, 0.0453, 0.0685, 0.0540, 0.0691, 0.0329, 0.0829],\n",
       "         [0.0473, 0.0737, 0.0909, 0.0755, 0.0426, 0.0424, 0.0847, 0.0291, 0.0514,\n",
       "          0.0486, 0.0331, 0.0293, 0.0269, 0.1510, 0.0641, 0.1094],\n",
       "         [0.0735, 0.0331, 0.0664, 0.0841, 0.0705, 0.0420, 0.0902, 0.0536, 0.0678,\n",
       "          0.0629, 0.0514, 0.0717, 0.0849, 0.0725, 0.0333, 0.0419],\n",
       "         [0.0402, 0.0600, 0.0425, 0.0861, 0.0590, 0.0740, 0.0652, 0.0882, 0.0653,\n",
       "          0.0829, 0.0826, 0.0300, 0.0481, 0.0559, 0.0584, 0.0617],\n",
       "         [0.0906, 0.0695, 0.0354, 0.0740, 0.0870, 0.0877, 0.0677, 0.0476, 0.0424,\n",
       "          0.0708, 0.0662, 0.0386, 0.0406, 0.0559, 0.0788, 0.0473],\n",
       "         [0.0690, 0.0559, 0.0490, 0.0636, 0.0460, 0.1080, 0.0522, 0.0662, 0.0556,\n",
       "          0.0560, 0.0756, 0.0629, 0.0995, 0.0332, 0.0834, 0.0238],\n",
       "         [0.0795, 0.0564, 0.0562, 0.0347, 0.0911, 0.0656, 0.0934, 0.0618, 0.0331,\n",
       "          0.0852, 0.0435, 0.0734, 0.0658, 0.0476, 0.0579, 0.0547],\n",
       "         [0.0491, 0.0538, 0.1191, 0.0901, 0.0554, 0.0180, 0.0628, 0.0695, 0.0605,\n",
       "          0.0422, 0.0142, 0.0507, 0.0511, 0.1128, 0.0853, 0.0655],\n",
       "         [0.0723, 0.0459, 0.0538, 0.0662, 0.0923, 0.0574, 0.0895, 0.0682, 0.0480,\n",
       "          0.0565, 0.0613, 0.0637, 0.0626, 0.0680, 0.0622, 0.0322],\n",
       "         [0.0395, 0.0680, 0.0319, 0.1235, 0.0543, 0.0532, 0.0283, 0.0913, 0.0559,\n",
       "          0.0357, 0.0437, 0.0278, 0.0307, 0.0906, 0.1789, 0.0467],\n",
       "         [0.1825, 0.0296, 0.0505, 0.0396, 0.0422, 0.0837, 0.0508, 0.0245, 0.0617,\n",
       "          0.0601, 0.1123, 0.0787, 0.0655, 0.0331, 0.0361, 0.0491],\n",
       "         [0.0504, 0.0546, 0.0488, 0.1148, 0.0426, 0.0387, 0.0812, 0.0531, 0.0702,\n",
       "          0.0662, 0.0435, 0.0243, 0.0468, 0.1147, 0.0826, 0.0675],\n",
       "         [0.0326, 0.0397, 0.0295, 0.1257, 0.0617, 0.0096, 0.0130, 0.0785, 0.0289,\n",
       "          0.0325, 0.0313, 0.2567, 0.0503, 0.0981, 0.0768, 0.0352],\n",
       "         [0.0709, 0.0574, 0.0650, 0.0383, 0.0635, 0.1082, 0.0917, 0.0504, 0.0401,\n",
       "          0.0598, 0.0494, 0.0807, 0.0652, 0.0621, 0.0365, 0.0609],\n",
       "         [0.0541, 0.0822, 0.0360, 0.0660, 0.0766, 0.0573, 0.0725, 0.0583, 0.0510,\n",
       "          0.0425, 0.0412, 0.0939, 0.0734, 0.0644, 0.0746, 0.0560],\n",
       "         [0.1067, 0.0396, 0.0677, 0.0835, 0.0264, 0.0796, 0.0604, 0.0514, 0.0787,\n",
       "          0.0302, 0.0836, 0.0357, 0.1579, 0.0350, 0.0298, 0.0338],\n",
       "         [0.0585, 0.0563, 0.0431, 0.1078, 0.0743, 0.0305, 0.0438, 0.1014, 0.0587,\n",
       "          0.0278, 0.0926, 0.1029, 0.0495, 0.0598, 0.0594, 0.0336],\n",
       "         [0.0478, 0.0432, 0.0407, 0.2791, 0.0536, 0.0132, 0.0452, 0.0569, 0.0619,\n",
       "          0.0319, 0.0483, 0.0368, 0.0173, 0.0872, 0.0923, 0.0446]],\n",
       "        grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b09ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def One_hot(labels):\n",
    "    num_class=16\n",
    "    lb=LabelBinarizer().fit(np.array(range(num_class)))\n",
    "    labels=lb.transform(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37025fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_step: 144\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  #使用交叉熵作为损失函数\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "num_classes = 16\n",
    "learning_rate = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# 225\n",
    "total_step = len(train_loader)\n",
    "print(\"total_step:\", total_step)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e278658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [50/144], Loss: 2.1384, Accuracy: 74.00%\n",
      "Epoch [1/50], Step [100/144], Loss: 2.0981, Accuracy: 78.00%\n",
      "Epoch [2/50], Step [50/144], Loss: 2.0321, Accuracy: 84.00%\n",
      "Epoch [2/50], Step [100/144], Loss: 2.1178, Accuracy: 76.00%\n",
      "Epoch [3/50], Step [50/144], Loss: 2.3726, Accuracy: 50.00%\n",
      "Epoch [3/50], Step [100/144], Loss: 2.1820, Accuracy: 70.00%\n",
      "Epoch [4/50], Step [50/144], Loss: 2.2578, Accuracy: 62.00%\n",
      "Epoch [4/50], Step [100/144], Loss: 2.0172, Accuracy: 86.00%\n",
      "Epoch [5/50], Step [50/144], Loss: 2.1666, Accuracy: 72.00%\n",
      "Epoch [5/50], Step [100/144], Loss: 2.1500, Accuracy: 72.00%\n",
      "Epoch [6/50], Step [50/144], Loss: 2.1730, Accuracy: 70.00%\n",
      "Epoch [6/50], Step [100/144], Loss: 2.1390, Accuracy: 74.00%\n",
      "Epoch [7/50], Step [50/144], Loss: 2.0764, Accuracy: 80.00%\n",
      "Epoch [7/50], Step [100/144], Loss: 2.2264, Accuracy: 66.00%\n",
      "Epoch [8/50], Step [50/144], Loss: 2.1772, Accuracy: 70.00%\n",
      "Epoch [8/50], Step [100/144], Loss: 2.1259, Accuracy: 76.00%\n",
      "Epoch [9/50], Step [50/144], Loss: 2.1894, Accuracy: 68.00%\n",
      "Epoch [9/50], Step [100/144], Loss: 2.2008, Accuracy: 68.00%\n",
      "Epoch [10/50], Step [50/144], Loss: 2.1386, Accuracy: 74.00%\n",
      "Epoch [10/50], Step [100/144], Loss: 2.0757, Accuracy: 80.00%\n",
      "Epoch [11/50], Step [50/144], Loss: 2.0807, Accuracy: 80.00%\n",
      "Epoch [11/50], Step [100/144], Loss: 2.2192, Accuracy: 66.00%\n",
      "Epoch [12/50], Step [50/144], Loss: 2.1464, Accuracy: 72.00%\n",
      "Epoch [12/50], Step [100/144], Loss: 2.0834, Accuracy: 78.00%\n",
      "Epoch [13/50], Step [50/144], Loss: 2.1026, Accuracy: 78.00%\n",
      "Epoch [13/50], Step [100/144], Loss: 2.1232, Accuracy: 76.00%\n",
      "Epoch [14/50], Step [50/144], Loss: 2.1297, Accuracy: 74.00%\n",
      "Epoch [14/50], Step [100/144], Loss: 2.0997, Accuracy: 78.00%\n",
      "Epoch [15/50], Step [50/144], Loss: 2.2075, Accuracy: 66.00%\n",
      "Epoch [15/50], Step [100/144], Loss: 2.1420, Accuracy: 74.00%\n",
      "Epoch [16/50], Step [50/144], Loss: 2.1136, Accuracy: 76.00%\n",
      "Epoch [16/50], Step [100/144], Loss: 2.1195, Accuracy: 76.00%\n",
      "Epoch [17/50], Step [50/144], Loss: 2.3000, Accuracy: 58.00%\n",
      "Epoch [17/50], Step [100/144], Loss: 2.1954, Accuracy: 68.00%\n",
      "Epoch [18/50], Step [50/144], Loss: 2.1795, Accuracy: 70.00%\n",
      "Epoch [18/50], Step [100/144], Loss: 2.0945, Accuracy: 78.00%\n",
      "Epoch [19/50], Step [50/144], Loss: 2.2049, Accuracy: 68.00%\n",
      "Epoch [19/50], Step [100/144], Loss: 2.1014, Accuracy: 78.00%\n",
      "Epoch [20/50], Step [50/144], Loss: 2.1967, Accuracy: 68.00%\n",
      "Epoch [20/50], Step [100/144], Loss: 2.0798, Accuracy: 80.00%\n",
      "Epoch [21/50], Step [50/144], Loss: 2.0462, Accuracy: 84.00%\n",
      "Epoch [21/50], Step [100/144], Loss: 1.9967, Accuracy: 88.00%\n",
      "Epoch [22/50], Step [50/144], Loss: 2.1853, Accuracy: 70.00%\n",
      "Epoch [22/50], Step [100/144], Loss: 2.0589, Accuracy: 82.00%\n",
      "Epoch [23/50], Step [50/144], Loss: 2.2085, Accuracy: 66.00%\n",
      "Epoch [23/50], Step [100/144], Loss: 2.1326, Accuracy: 74.00%\n",
      "Epoch [24/50], Step [50/144], Loss: 2.2237, Accuracy: 66.00%\n",
      "Epoch [24/50], Step [100/144], Loss: 2.1668, Accuracy: 72.00%\n",
      "Epoch [25/50], Step [50/144], Loss: 2.1369, Accuracy: 74.00%\n",
      "Epoch [25/50], Step [100/144], Loss: 2.1741, Accuracy: 70.00%\n",
      "Epoch [26/50], Step [50/144], Loss: 2.2746, Accuracy: 60.00%\n",
      "Epoch [26/50], Step [100/144], Loss: 2.1770, Accuracy: 70.00%\n",
      "Epoch [27/50], Step [50/144], Loss: 2.1516, Accuracy: 72.00%\n",
      "Epoch [27/50], Step [100/144], Loss: 2.1352, Accuracy: 74.00%\n",
      "Epoch [28/50], Step [50/144], Loss: 2.1568, Accuracy: 72.00%\n",
      "Epoch [28/50], Step [100/144], Loss: 2.1642, Accuracy: 70.00%\n",
      "Epoch [29/50], Step [50/144], Loss: 2.1391, Accuracy: 74.00%\n",
      "Epoch [29/50], Step [100/144], Loss: 2.1641, Accuracy: 72.00%\n",
      "Epoch [30/50], Step [50/144], Loss: 2.1325, Accuracy: 74.00%\n",
      "Epoch [30/50], Step [100/144], Loss: 2.0983, Accuracy: 78.00%\n",
      "Epoch [31/50], Step [50/144], Loss: 2.1170, Accuracy: 76.00%\n",
      "Epoch [31/50], Step [100/144], Loss: 2.1801, Accuracy: 70.00%\n",
      "Epoch [32/50], Step [50/144], Loss: 2.0163, Accuracy: 86.00%\n",
      "Epoch [32/50], Step [100/144], Loss: 2.0599, Accuracy: 82.00%\n",
      "Epoch [33/50], Step [50/144], Loss: 2.1718, Accuracy: 70.00%\n",
      "Epoch [33/50], Step [100/144], Loss: 2.1773, Accuracy: 70.00%\n",
      "Epoch [34/50], Step [50/144], Loss: 2.1844, Accuracy: 70.00%\n",
      "Epoch [34/50], Step [100/144], Loss: 2.1870, Accuracy: 68.00%\n",
      "Epoch [35/50], Step [50/144], Loss: 2.1940, Accuracy: 68.00%\n",
      "Epoch [35/50], Step [100/144], Loss: 2.1512, Accuracy: 72.00%\n",
      "Epoch [36/50], Step [50/144], Loss: 2.2617, Accuracy: 62.00%\n",
      "Epoch [36/50], Step [100/144], Loss: 2.1563, Accuracy: 72.00%\n",
      "Epoch [37/50], Step [50/144], Loss: 2.2180, Accuracy: 66.00%\n",
      "Epoch [37/50], Step [100/144], Loss: 2.1937, Accuracy: 68.00%\n",
      "Epoch [38/50], Step [50/144], Loss: 2.2155, Accuracy: 66.00%\n",
      "Epoch [38/50], Step [100/144], Loss: 2.1605, Accuracy: 72.00%\n",
      "Epoch [39/50], Step [50/144], Loss: 2.1847, Accuracy: 68.00%\n",
      "Epoch [39/50], Step [100/144], Loss: 2.1663, Accuracy: 70.00%\n",
      "Epoch [40/50], Step [50/144], Loss: 2.1019, Accuracy: 78.00%\n",
      "Epoch [40/50], Step [100/144], Loss: 2.1177, Accuracy: 76.00%\n",
      "Epoch [41/50], Step [50/144], Loss: 2.1555, Accuracy: 72.00%\n",
      "Epoch [41/50], Step [100/144], Loss: 2.1068, Accuracy: 78.00%\n",
      "Epoch [42/50], Step [50/144], Loss: 2.0659, Accuracy: 82.00%\n",
      "Epoch [42/50], Step [100/144], Loss: 2.0835, Accuracy: 80.00%\n",
      "Epoch [43/50], Step [50/144], Loss: 2.1209, Accuracy: 74.00%\n",
      "Epoch [43/50], Step [100/144], Loss: 2.2819, Accuracy: 58.00%\n",
      "Epoch [44/50], Step [50/144], Loss: 2.1568, Accuracy: 72.00%\n",
      "Epoch [44/50], Step [100/144], Loss: 2.0949, Accuracy: 78.00%\n",
      "Epoch [45/50], Step [50/144], Loss: 2.0885, Accuracy: 76.00%\n",
      "Epoch [45/50], Step [100/144], Loss: 2.2559, Accuracy: 62.00%\n",
      "Epoch [46/50], Step [50/144], Loss: 2.1399, Accuracy: 74.00%\n",
      "Epoch [46/50], Step [100/144], Loss: 2.0765, Accuracy: 80.00%\n",
      "Epoch [47/50], Step [50/144], Loss: 2.1951, Accuracy: 68.00%\n",
      "Epoch [47/50], Step [100/144], Loss: 2.1353, Accuracy: 74.00%\n",
      "Epoch [48/50], Step [50/144], Loss: 2.2567, Accuracy: 62.00%\n",
      "Epoch [48/50], Step [100/144], Loss: 2.0574, Accuracy: 82.00%\n",
      "Epoch [49/50], Step [50/144], Loss: 2.0602, Accuracy: 82.00%\n",
      "Epoch [49/50], Step [100/144], Loss: 2.0513, Accuracy: 82.00%\n",
      "Epoch [50/50], Step [50/144], Loss: 2.1399, Accuracy: 74.00%\n",
      "Epoch [50/50], Step [100/144], Loss: 2.1913, Accuracy: 68.00%\n",
      "----------------模型经过训练!-------------------- \n"
     ]
    }
   ],
   "source": [
    "tf_iter = 0 \n",
    "batch_size_train = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.permute(1,2,0)\n",
    "        labels = labels.squeeze()\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        \n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # 反向传播和优化器\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 精度\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        accuracy = (correct / total) * 100\n",
    "        \n",
    "#         writer.add_scalar('Accuracy/train',  accuracy,  tf_iter)\n",
    "#         writer.add_scalar('Loss/train',  loss.item(),  tf_iter)\n",
    "        tf_iter += 1\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, (i + 1)*batch_size_train,\n",
    "                  total_step, loss.item(),\n",
    "                  accuracy))\n",
    "    \n",
    "print(\"----------------模型经过训练!-------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6a7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 32\n",
    "# 在训练集上循环训练\n",
    "for epoch in range(2):\n",
    "    # 损失值\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # 获得输入张量和标签\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.permute(1,2,0)\n",
    "        labels = labels.squeeze()\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        # 模型梯度归零，否则会不断累积。optimizer.zero_grad()也可以\n",
    "        cnn.zero_grad()\n",
    "        # 数据经过模型计算\n",
    "        labels = labels.squeeze()\n",
    "        outputs = cnn(inputs)\n",
    "        # 计算模型预测与实际标签的损失值\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 梯度自动反向传播\n",
    "        loss.backward()\n",
    "        # 优化器执行优化，按梯度下降原则调整参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        losses = []\n",
    "        # 每1000轮输出一次损失值\n",
    "        if batch_i != 0 and batch_i % print_every == 0:\n",
    "            avg_train_loss = running_loss/print_every\n",
    "            losses.append(avg_train_loss)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n",
    "            running_loss = 0 # reset accumulated training loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1404c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混淆矩阵：\n",
      "[[1179    0    1    0    0    0    3    1    0    0    0    7    0    0\n",
      "     0    0]\n",
      " [   0   91    1    0    0    0    1    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   1    1   56    4    0    6    0    0    7    6    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    7   36    0   17    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  93    0    0    0    0    2    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   1    0    6   22    0   23    1    0   39    0    0    0    0    0\n",
      "     1    0]\n",
      " [  17    3    0    0    0    1   53    0    6    0    0    1    0    0\n",
      "     0    0]\n",
      " [  82    0    0    0    0    0    0    0    0    0    1    0    0    0\n",
      "     0    0]\n",
      " [   0    0    4    0    0    0    1    0   62    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0   26    1    0    0    0    0    0   60    0    2    0    0\n",
      "     0    0]\n",
      " [  70    0    0    0    0    0    1    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  80    0    0    0    0    0    2    0    0    1    0   15    0    0\n",
      "     0    0]\n",
      " [  70    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  71    0    0    0    0    0    1    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  68    0    0    2    0    1    0    0    0    0    0    8    0    0\n",
      "     1    0]\n",
      " [  73    0    0    0    0    0    2    0    0    0    0    1    0    0\n",
      "     0    0]]\n",
      "测试集的准确率: 65.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# 训练后的模型测试效果\n",
    "predict_all = np.array([], dtype=int)\n",
    "labels_all = np.array([], dtype=int)\n",
    "# 测试预测时取消自动梯度，即向前计算时不自动求导\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.unsqueeze(0)\n",
    "        data = data.permute(1,2,0)\n",
    "        outputs = cnn(data)\n",
    "        labels = labels.data.numpy()\n",
    "        predicted = torch.max(outputs.data, 1)[1].numpy()\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "        predict_all = np.append(predict_all, predicted)\n",
    "\n",
    "confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "print(u'混淆矩阵：')\n",
    "print(confusion)\n",
    "# 验证集准确度\n",
    "acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "print(u'测试集的准确率: {0:.2%}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59100139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
